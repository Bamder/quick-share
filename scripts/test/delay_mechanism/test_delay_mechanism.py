"""
å»¶æ—¶æœºåˆ¶æµ‹è¯•

æµ‹è¯•æ–‡ä»¶å¤ç”¨æ—¶çš„å»¶æ—¶æœºåˆ¶ï¼š
- å¤ç”¨æ–‡ä»¶ç¼“å­˜æ—¶å»¶é•¿è¿‡æœŸæ—¶é—´

ä½¿ç”¨æ–¹æ³•:
    python scripts/test/delay_mechanism/test_delay_mechanism.py
"""

import os
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

def check_venv():
    in_venv = (
        hasattr(sys, 'real_prefix') or
        (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix) or
        os.environ.get('VIRTUAL_ENV') is not None
    )
    if not in_venv:
        print("è­¦å‘Š: æœªæ£€æµ‹åˆ°è™šæ‹Ÿç¯å¢ƒ")
        # åœ¨éäº¤äº’å¼ç¯å¢ƒä¸­è‡ªåŠ¨ç»§ç»­
        if not sys.stdin.isatty():
            print("éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨ç»§ç»­...")
            return False
        try:
            if input("ç»§ç»­? (y/n): ").strip().lower() != 'y':
                sys.exit(0)
        except (EOFError, KeyboardInterrupt):
            print("è¾“å…¥å–æ¶ˆï¼Œé€€å‡ºæµ‹è¯•")
            sys.exit(0)
    return in_venv

check_venv()

from app.utils.pickup_code import DatetimeUtil
from datetime import datetime, timedelta
import logging

sys.path.insert(0, str(Path(__file__).parent.parent))
from test_utils import *

# é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°
logging.basicConfig(level=logging.INFO, format='%(message)s')

logger = logging.getLogger(__name__)


def test_delay_extension_basic():
    """æµ‹è¯•åŸºæœ¬å»¶æ—¶å»¶é•¿æœºåˆ¶ï¼šæ–°å–ä»¶ç è¿‡æœŸæ—¶é—´æ›´æ™š"""
    log_test_start("åŸºæœ¬å»¶æ—¶å»¶é•¿æœºåˆ¶")

    try:
        from app.services.cache_service import chunk_cache, file_info_cache, encrypted_key_cache
        from app.services.mapping_service import update_cache_expire_at
        from app.extensions import SessionLocal
        from app.models.user import User
        from app.models.file import File
        from app.models.pickup_code import PickupCode
        from app.utils.pickup_code import generate_unique_pickup_code

        db = SessionLocal()

        try:
            # 1. åˆ›å»ºæµ‹è¯•ç”¨æˆ·å’Œæ–‡ä»¶
            user = User(username="test_delay_user_1", password_hash="dummy_hash")
            db.add(user)
            db.flush()

            file_record = File(
                original_name="test_delay_file.txt",
                stored_name="stored_delay_file",
                size=1024,
                hash="delay_test_hash_1",
                mime_type="text/plain",
                uploader_id=user.id
            )
            db.add(file_record)
            db.flush()

            # 2. åˆ›å»ºåˆå§‹å–ä»¶ç ï¼ˆæ ‡è¯†ç ï¼‰
            original_lookup_code, _ = generate_unique_pickup_code(db)
            original_expire_at = DatetimeUtil.now() + timedelta(hours=1)

            pickup_code = PickupCode(
                code=original_lookup_code,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=original_expire_at,
                created_at=DatetimeUtil.now()
            )
            db.add(pickup_code)
            db.commit()

            # 3. è®¾ç½®åˆå§‹ç¼“å­˜æ•°æ®
            chunks = {
                0: {
                    'data': b'test_chunk_data',
                    'hash': 'test_hash',
                    'pickup_expire_at': original_expire_at,
                    'expires_at': original_expire_at,
                }
            }
            file_info = {
                'fileName': 'test_delay_file.txt',
                'fileSize': 1024,
                'mimeType': 'text/plain',
                'totalChunks': 1,
                'uploadedAt': DatetimeUtil.now(),
                'pickup_expire_at': original_expire_at,
            }

            chunk_cache.set(original_lookup_code, chunks, user.id)
            file_info_cache.set(original_lookup_code, file_info, user.id)
            encrypted_key_cache.set(original_lookup_code, "encrypted_key_1", user.id, original_expire_at)

            # 4. éªŒè¯åˆå§‹ç¼“å­˜è®¾ç½®
            if not chunk_cache.exists(original_lookup_code, user.id):
                log_error("åˆå§‹æ–‡ä»¶å—ç¼“å­˜è®¾ç½®å¤±è´¥")
                return False

            initial_chunks = chunk_cache.get(original_lookup_code, user.id)
            initial_expire = initial_chunks[0]['expires_at']
            log_info(f"åˆå§‹è¿‡æœŸæ—¶é—´: {initial_expire}")

            # 5. åˆ›å»ºæ–°å–ä»¶ç ï¼ˆæ¨¡æ‹Ÿæ–‡ä»¶å¤ç”¨ï¼‰
            new_lookup_code, _ = generate_unique_pickup_code(db)
            new_expire_at = DatetimeUtil.now() + timedelta(hours=2)  # æ›´æ™šçš„è¿‡æœŸæ—¶é—´

            new_pickup_code = PickupCode(
                code=new_lookup_code,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=new_expire_at,
                created_at=DatetimeUtil.now()
            )
            db.add(new_pickup_code)
            db.commit()

            # 6. æ‰§è¡Œå»¶æ—¶å»¶é•¿ï¼ˆæ¨¡æ‹Ÿå¤ç”¨æ—¶çš„ç¼“å­˜æ›´æ–°ï¼‰
            update_cache_expire_at(original_lookup_code, new_expire_at, db, user.id)

            # 7. éªŒè¯ç¼“å­˜è¿‡æœŸæ—¶é—´å·²è¢«å»¶é•¿
            updated_chunks = chunk_cache.get(original_lookup_code, user.id)
            updated_expire = updated_chunks[0]['expires_at']
            updated_file_info = file_info_cache.get(original_lookup_code, user.id)
            updated_file_info_expire = updated_file_info.get('pickup_expire_at')

            log_info(f"æ›´æ–°åè¿‡æœŸæ—¶é—´: chunk={updated_expire}, file_info={updated_file_info_expire}")

            # æ£€æŸ¥è¿‡æœŸæ—¶é—´æ˜¯å¦è¢«å»¶é•¿
            if updated_expire >= new_expire_at and updated_file_info_expire >= new_expire_at:
                log_success("æ–‡ä»¶å—å’Œæ–‡ä»¶ä¿¡æ¯ç¼“å­˜è¿‡æœŸæ—¶é—´æˆåŠŸå»¶é•¿")
                success = True
            else:
                log_error(f"ç¼“å­˜è¿‡æœŸæ—¶é—´æœªå»¶é•¿: chunk={updated_expire}, file_info={updated_file_info_expire}, expected={new_expire_at}")
                success = False

            # 8. éªŒè¯å¯†é’¥ç¼“å­˜æœªè¢«å»¶æ—¶ï¼ˆåº”è¯¥ä¿æŒåŸå§‹è¿‡æœŸæ—¶é—´ï¼‰
            # æ³¨æ„ï¼šå¯†é’¥ç¼“å­˜ä½¿ç”¨ lookup_code ä½œä¸ºé”®ï¼Œæ¯ä¸ªå–ä»¶ç ç‹¬ç«‹å­˜å‚¨
            # æ‰€ä»¥ original_lookup_code çš„å¯†é’¥ç¼“å­˜åº”è¯¥ä¿æŒåŸå§‹è¿‡æœŸæ—¶é—´
            # ä½†è¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥æ£€æŸ¥è¿‡æœŸæ—¶é—´ï¼Œåªèƒ½æ£€æŸ¥æ˜¯å¦å­˜åœ¨
            if encrypted_key_cache.exists(original_lookup_code, user.id):
                log_success("å¯†é’¥ç¼“å­˜å­˜åœ¨ï¼ˆç‹¬ç«‹å­˜å‚¨ï¼Œä¸åº”è¢«å»¶æ—¶ï¼‰")
            else:
                log_error("å¯†é’¥ç¼“å­˜ä¸å­˜åœ¨ï¼ˆå¯èƒ½è¢«é”™è¯¯æ¸…ç†ï¼‰")
                success = False

            # 9. æ¸…ç†æµ‹è¯•æ•°æ®
            chunk_cache.delete(original_lookup_code, user.id)
            file_info_cache.delete(original_lookup_code, user.id)
            encrypted_key_cache.delete(original_lookup_code, user.id)

            db.query(PickupCode).filter(PickupCode.code.in_([original_lookup_code, new_lookup_code])).delete()
            db.query(File).filter(File.id == file_record.id).delete()
            db.query(User).filter(User.id == user.id).delete()
            db.commit()

            return success

        finally:
            db.close()

    except Exception as e:
        log_error(f"åŸºæœ¬å»¶æ—¶å»¶é•¿æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_delay_extension_multiple_codes():
    """æµ‹è¯•å¤šä¸ªå–ä»¶ç å…±äº«æ ‡è¯†ç æ—¶çš„å»¶æ—¶ï¼šåº”è¯¥å–æœ€æ™šçš„è¿‡æœŸæ—¶é—´"""
    log_test_start("å¤šä¸ªå–ä»¶ç å…±äº«æ ‡è¯†ç æ—¶çš„å»¶æ—¶")

    try:
        from app.services.cache_service import chunk_cache, file_info_cache
        from app.services.mapping_service import update_cache_expire_at, save_lookup_mapping
        from app.extensions import SessionLocal
        from app.models.user import User
        from app.models.file import File
        from app.models.pickup_code import PickupCode
        from app.utils.pickup_code import generate_unique_pickup_code

        db = SessionLocal()

        try:
            # 1. åˆ›å»ºæµ‹è¯•ç”¨æˆ·å’Œæ–‡ä»¶
            user = User(username="test_delay_user_2", password_hash="dummy_hash")
            db.add(user)
            db.flush()

            file_record = File(
                original_name="test_delay_file_2.txt",
                stored_name="stored_delay_file_2",
                size=2048,
                hash="delay_test_hash_2",
                mime_type="text/plain",
                uploader_id=user.id
            )
            db.add(file_record)
            db.flush()

            # 2. åˆ›å»ºç¬¬ä¸€ä¸ªå–ä»¶ç ï¼ˆæ ‡è¯†ç ï¼‰
            original_lookup_code, _ = generate_unique_pickup_code(db)
            expire_at_1 = DatetimeUtil.now() + timedelta(hours=1)

            pickup_code_1 = PickupCode(
                code=original_lookup_code,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=expire_at_1,
                created_at=DatetimeUtil.now()
            )
            db.add(pickup_code_1)
            db.commit()

            # 3. è®¾ç½®åˆå§‹ç¼“å­˜æ•°æ®
            chunks = {
                0: {
                    'data': b'test_chunk_data_2',
                    'hash': 'test_hash_2',
                    'pickup_expire_at': expire_at_1,
                    'expires_at': expire_at_1,
                }
            }
            file_info = {
                'fileName': 'test_delay_file_2.txt',
                'fileSize': 2048,
                'mimeType': 'text/plain',
                'totalChunks': 1,
                'uploadedAt': DatetimeUtil.now(),
                'pickup_expire_at': expire_at_1,
            }

            chunk_cache.set(original_lookup_code, chunks, user.id)
            file_info_cache.set(original_lookup_code, file_info, user.id)

            # 4. åˆ›å»ºç¬¬äºŒä¸ªå–ä»¶ç ï¼ˆè¿‡æœŸæ—¶é—´æ›´æ™šï¼‰
            lookup_code_2, _ = generate_unique_pickup_code(db)
            expire_at_2 = DatetimeUtil.now() + timedelta(hours=3)  # æ›´æ™šçš„è¿‡æœŸæ—¶é—´

            pickup_code_2 = PickupCode(
                code=lookup_code_2,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=expire_at_2,
                created_at=DatetimeUtil.now()
            )
            db.add(pickup_code_2)
            db.commit()

            # ä¿å­˜æ˜ å°„å…³ç³»
            save_lookup_mapping(lookup_code_2, original_lookup_code, expire_at_2)

            # 5. æ›´æ–°ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆåº”è¯¥ä½¿ç”¨ expire_at_2ï¼Œå› ä¸ºå®ƒæ›´æ™šï¼‰
            update_cache_expire_at(original_lookup_code, expire_at_2, db, user.id)

            # 6. åˆ›å»ºç¬¬ä¸‰ä¸ªå–ä»¶ç ï¼ˆè¿‡æœŸæ—¶é—´æ›´æ—©ï¼‰
            lookup_code_3, _ = generate_unique_pickup_code(db)
            expire_at_3 = DatetimeUtil.now() + timedelta(hours=2)  # æ¯” expire_at_2 æ—©

            pickup_code_3 = PickupCode(
                code=lookup_code_3,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=expire_at_3,
                created_at=DatetimeUtil.now()
            )
            db.add(pickup_code_3)
            db.commit()

            # ä¿å­˜æ˜ å°„å…³ç³»
            save_lookup_mapping(lookup_code_3, original_lookup_code, expire_at_3)

            # 7. å†æ¬¡æ›´æ–°ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆåº”è¯¥ä¿æŒ expire_at_2ï¼Œå› ä¸ºå®ƒæ˜¯æ‰€æœ‰å–ä»¶ç ä¸­æœ€æ™šçš„ï¼‰
            update_cache_expire_at(original_lookup_code, expire_at_3, db, user.id)

            # 8. éªŒè¯ç¼“å­˜è¿‡æœŸæ—¶é—´åº”è¯¥æ˜¯æœ€æ™šçš„ï¼ˆexpire_at_2ï¼‰
            updated_chunks = chunk_cache.get(original_lookup_code, user.id)
            updated_expire = updated_chunks[0]['expires_at']
            updated_file_info = file_info_cache.get(original_lookup_code, user.id)
            updated_file_info_expire = updated_file_info.get('pickup_expire_at')

            log_info(f"è¿‡æœŸæ—¶é—´: chunk={updated_expire}, file_info={updated_file_info_expire}")
            log_info(f"æœŸæœ›: {expire_at_2} (æœ€æ™šçš„è¿‡æœŸæ—¶é—´)")

            # æ£€æŸ¥è¿‡æœŸæ—¶é—´åº”è¯¥æ˜¯æœ€æ™šçš„ï¼ˆexpire_at_2ï¼‰ï¼Œè€Œä¸æ˜¯ expire_at_3
            if updated_expire >= expire_at_2 and updated_file_info_expire >= expire_at_2:
                log_success("ç¼“å­˜è¿‡æœŸæ—¶é—´æ­£ç¡®ä½¿ç”¨æœ€æ™šçš„è¿‡æœŸæ—¶é—´")
                success = True
            else:
                log_error(f"ç¼“å­˜è¿‡æœŸæ—¶é—´é”™è¯¯: æœŸæœ›>={expire_at_2}, å®é™… chunk={updated_expire}, file_info={updated_file_info_expire}")
                success = False

            # 9. æ¸…ç†æµ‹è¯•æ•°æ®
            chunk_cache.delete(original_lookup_code, user.id)
            file_info_cache.delete(original_lookup_code, user.id)

            db.query(PickupCode).filter(PickupCode.code.in_([original_lookup_code, lookup_code_2, lookup_code_3])).delete()
            db.query(File).filter(File.id == file_record.id).delete()
            db.query(User).filter(User.id == user.id).delete()
            db.commit()

            return success

        finally:
            db.close()

    except Exception as e:
        log_error(f"å¤šä¸ªå–ä»¶ç å»¶æ—¶æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_delay_extension_no_shorten():
    """æµ‹è¯•å»¶æ—¶æœºåˆ¶ä¸ä¼šç¼©çŸ­è¿‡æœŸæ—¶é—´ï¼šæ–°å–ä»¶ç è¿‡æœŸæ—¶é—´æ›´æ—©æ—¶ï¼Œåº”è¯¥ä¿æŒç°æœ‰è¿‡æœŸæ—¶é—´"""
    log_test_start("å»¶æ—¶æœºåˆ¶ä¸ç¼©çŸ­è¿‡æœŸæ—¶é—´")

    try:
        from app.services.cache_service import chunk_cache, file_info_cache
        from app.services.mapping_service import update_cache_expire_at
        from app.extensions import SessionLocal
        from app.models.user import User
        from app.models.file import File
        from app.models.pickup_code import PickupCode
        from app.utils.pickup_code import generate_unique_pickup_code

        db = SessionLocal()

        try:
            # 1. åˆ›å»ºæµ‹è¯•ç”¨æˆ·å’Œæ–‡ä»¶
            user = User(username="test_delay_user_3", password_hash="dummy_hash")
            db.add(user)
            db.flush()

            file_record = File(
                original_name="test_delay_file_3.txt",
                stored_name="stored_delay_file_3",
                size=3072,
                hash="delay_test_hash_3",
                mime_type="text/plain",
                uploader_id=user.id
            )
            db.add(file_record)
            db.flush()

            # 2. åˆ›å»ºåˆå§‹å–ä»¶ç ï¼ˆæ ‡è¯†ç ï¼‰
            original_lookup_code, _ = generate_unique_pickup_code(db)
            original_expire_at = DatetimeUtil.now() + timedelta(hours=3)  # è¾ƒæ™šçš„è¿‡æœŸæ—¶é—´

            pickup_code = PickupCode(
                code=original_lookup_code,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=original_expire_at,
                created_at=DatetimeUtil.now()
            )
            db.add(pickup_code)
            db.commit()

            # 3. è®¾ç½®åˆå§‹ç¼“å­˜æ•°æ®
            chunks = {
                0: {
                    'data': b'test_chunk_data_3',
                    'hash': 'test_hash_3',
                    'pickup_expire_at': original_expire_at,
                    'expires_at': original_expire_at,
                }
            }
            file_info = {
                'fileName': 'test_delay_file_3.txt',
                'fileSize': 3072,
                'mimeType': 'text/plain',
                'totalChunks': 1,
                'uploadedAt': DatetimeUtil.now(),
                'pickup_expire_at': original_expire_at,
            }

            chunk_cache.set(original_lookup_code, chunks, user.id)
            file_info_cache.set(original_lookup_code, file_info, user.id)

            # 4. åˆ›å»ºæ–°å–ä»¶ç ï¼ˆè¿‡æœŸæ—¶é—´æ›´æ—©ï¼‰
            new_lookup_code, _ = generate_unique_pickup_code(db)
            new_expire_at = DatetimeUtil.now() + timedelta(hours=1)  # æ›´æ—©çš„è¿‡æœŸæ—¶é—´

            new_pickup_code = PickupCode(
                code=new_lookup_code,
                file_id=file_record.id,
                status="waiting",
                used_count=0,
                limit_count=3,
                expire_at=new_expire_at,
                created_at=DatetimeUtil.now()
            )
            db.add(new_pickup_code)
            db.commit()

            # 5. æ‰§è¡Œå»¶æ—¶æ›´æ–°ï¼ˆåº”è¯¥ä¿æŒ original_expire_atï¼Œå› ä¸ºå®ƒæ˜¯æ›´æ™šçš„ï¼‰
            update_cache_expire_at(original_lookup_code, new_expire_at, db, user.id)

            # 6. éªŒè¯ç¼“å­˜è¿‡æœŸæ—¶é—´åº”è¯¥ä¿æŒ original_expire_atï¼ˆä¸åº”è¯¥ç¼©çŸ­ï¼‰
            updated_chunks = chunk_cache.get(original_lookup_code, user.id)
            updated_expire = updated_chunks[0]['expires_at']
            updated_file_info = file_info_cache.get(original_lookup_code, user.id)
            updated_file_info_expire = updated_file_info.get('pickup_expire_at')

            log_info(f"è¿‡æœŸæ—¶é—´: chunk={updated_expire}, file_info={updated_file_info_expire}")
            log_info(f"æœŸæœ›: {original_expire_at} (ä¸åº”è¯¥ç¼©çŸ­)")

            # æ£€æŸ¥è¿‡æœŸæ—¶é—´åº”è¯¥ä¿æŒ original_expire_atï¼ˆä¸åº”è¯¥ç¼©çŸ­åˆ° new_expire_atï¼‰
            if updated_expire >= original_expire_at and updated_file_info_expire >= original_expire_at:
                log_success("ç¼“å­˜è¿‡æœŸæ—¶é—´æ­£ç¡®ä¿æŒï¼ˆæœªç¼©çŸ­ï¼‰")
                success = True
            else:
                log_error(f"ç¼“å­˜è¿‡æœŸæ—¶é—´è¢«é”™è¯¯ç¼©çŸ­: æœŸæœ›>={original_expire_at}, å®é™… chunk={updated_expire}, file_info={updated_file_info_expire}")
                success = False

            # 7. æ¸…ç†æµ‹è¯•æ•°æ®
            chunk_cache.delete(original_lookup_code, user.id)
            file_info_cache.delete(original_lookup_code, user.id)

            db.query(PickupCode).filter(PickupCode.code.in_([original_lookup_code, new_lookup_code])).delete()
            db.query(File).filter(File.id == file_record.id).delete()
            db.query(User).filter(User.id == user.id).delete()
            db.commit()

            return success

        finally:
            db.close()

    except Exception as e:
        log_error(f"ä¸ç¼©çŸ­è¿‡æœŸæ—¶é—´æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_delay_mechanism_tests():
    """è¿è¡Œå»¶æ—¶æœºåˆ¶æµ‹è¯•"""
    log_section("å»¶æ—¶æœºåˆ¶æµ‹è¯•")

    tests = [
        ("å»¶æ—¶å»¶é•¿æµ‹è¯•", [
            test_delay_extension_basic,
            test_delay_extension_multiple_codes,
            test_delay_extension_no_shorten,
        ]),
    ]

    total_passed = 0
    total_tests = 0

    for section_name, section_tests in tests:
        log_subsection(f"{section_name} ({len(section_tests)} ä¸ªæµ‹è¯•)")

        section_passed = 0
        for test_func in section_tests:
            try:
                if test_func():
                    section_passed += 1
                    total_passed += 1
                total_tests += 1
            except Exception as e:
                log_error(f"æµ‹è¯•å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                total_tests += 1

        success_rate = (section_passed / len(section_tests) * 100) if section_tests else 0
        log_info(f"{section_name} é€šè¿‡: {section_passed}/{len(section_tests)} ({success_rate:.1f}%)")

    log_separator("æµ‹è¯•ç»“æœæ±‡æ€»")
    success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
    log_info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    log_info(f"é€šè¿‡æµ‹è¯•: {total_passed}")
    log_info(f"å¤±è´¥æµ‹è¯•: {total_tests - total_passed}")
    log_info(f"æˆåŠŸç‡: {success_rate:.1f}%")
    if total_passed == total_tests:
        log_success("æ‰€æœ‰å»¶æ—¶æœºåˆ¶æµ‹è¯•é€šè¿‡ï¼ğŸ‰")
    else:
        log_error("éƒ¨åˆ†å»¶æ—¶æœºåˆ¶æµ‹è¯•å¤±è´¥")

    return total_passed == total_tests


if __name__ == "__main__":
    success = run_delay_mechanism_tests()
    sys.exit(0 if success else 1)
